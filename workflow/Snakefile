from os import getcwd, makedirs, walk
from os.path import basename, exists, join
from tempfile import gettempdir
from shutil import rmtree
from urllib.parse import urlparse

import numpy as np
import pandas as pd

CONFIG_DIR = "config"
DATA_DIR = "data"
LOG_DIR = "logs"
RESOURCES_DIR = "resources"
RESULTS_DIR = "results"
RULES_DIR = "rules"
REPORT_DIR = "report"


configfile: join(CONFIG_DIR, "config.yaml")


STUDY_NAME = config["study"]

TEMP_DIR = config["tmp_dir"] if config["tmp_dir"] else gettempdir()

EXPAND_PARAMS = {}

PUBLIC_WRAPPERS_VERSION = config["wrappers"]["public"]["version"]
PERSONAL_WRAPPERS_BASE_URL = config["wrappers"]["personal"]["base_url"]
LOCAL_WRAPPERS_BASE_URL = config["wrappers"]["local"]["base_url"]

INPUT_MODE = config["input"]["mode"]

HOST_REF_FASTA_URL = config["resources"]["ref"]["fasta_url"]
HOST_REF_FASTA_FILE = join(
    RESOURCES_DIR, "ref", basename(urlparse(HOST_REF_FASTA_URL).path)
)

GDC_METADATA_DIR = config["input"]["gdc"]["metadata"]["data_dir"]
GDC_METADATA_LOG_DIR = config["input"]["gdc"]["metadata"]["log_dir"]
if INPUT_MODE == "gdc":
    GDC_BAM_META_FILENAME = config["input"]["gdc"]["metadata"]["file_meta_filename"]
    GDC_BAM_META_FILE = join(GDC_METADATA_DIR, GDC_BAM_META_FILENAME)
    GDC_READGRP_META_FILENAME = config["input"]["gdc"]["metadata"][
        "readgrp_meta_filename"
    ]
    GDC_READGRP_META_FILE = join(GDC_METADATA_DIR, GDC_READGRP_META_FILENAME)

    makedirs(GDC_METADATA_LOG_DIR, mode=0o755, exist_ok=True)
    GDC_METADATA_LOG = join(GDC_METADATA_LOG_DIR, "gdc_metadata.log")
    with open(GDC_METADATA_LOG, "wt") as log_fh:
        if not exists(GDC_BAM_META_FILE) or not exists(GDC_READGRP_META_FILE):
            log_fh.write("Getting GDC metadata...\n")
            makedirs(GDC_METADATA_DIR, mode=0o755, exist_ok=True)
            shell("Rscript ./workflow/scripts/gdc_metadata.R")
        else:
            log_fh.write("Using existing GDC metadata\n")

    GDC_BAM_META_DF = pd.read_csv(GDC_BAM_META_FILE, sep="\t").set_index(
        "file_id", drop=False, verify_integrity=True
    )
    GDC_READGRP_META_DF = pd.read_csv(
        GDC_READGRP_META_FILE, sep="\t", dtype={"is_paired_end": bool}
    ).set_index("read_group_id", drop=False, verify_integrity=True)

    GDC_RESULTS_DIR = join(RESULTS_DIR, "gdc")
    GDC_LOG_DIR = join(LOG_DIR, "gdc")
    GDC_FILE_RESULTS_DIR = join(GDC_RESULTS_DIR, "{method}", "{bam_id}")
    GDC_FILE_LOG_DIR = join(GDC_LOG_DIR, "{method}", "{bam_id}")
    GDC_UNMAPPED_BAM_FILE = join(GDC_FILE_RESULTS_DIR, "{bam_id}_unmapped.bam")
    GDC_UNMAPPED_BAM_LOG = join(GDC_FILE_LOG_DIR, "{bam_id}_unmapped_bam.log")
    GDC_UNMAPPED_BAM_FILES = GDC_BAM_META_DF.apply(
        lambda x: (
            join(
                GDC_RESULTS_DIR,
                "sg",
                x["file_id"],
                f"{x['file_id']}_unmapped.bam",
            )
            if x["num_uniq_read_groups"] == 1
            else join(
                GDC_RESULTS_DIR,
                "rg",
                x["file_id"],
                f"{x['file_id']}_unmapped.bam",
            )
        ),
        axis=1,
    ).tolist()
    GDC_UNMAPPED_FASTQ_R1_FILE = join(
        GDC_FILE_RESULTS_DIR, "{rg_id}_unmapped_r1.fastq.gz"
    )
    GDC_UNMAPPED_FASTQ_R2_FILE = join(
        GDC_FILE_RESULTS_DIR, "{rg_id}_unmapped_r2.fastq.gz"
    )
    GDC_UNMAPPED_FASTQ_O1_FILE = join(
        GDC_FILE_RESULTS_DIR, "{rg_id}_unmapped_o1.fastq.gz"
    )
    GDC_UNMAPPED_FASTQ_O2_FILE = join(
        GDC_FILE_RESULTS_DIR, "{rg_id}_unmapped_o2.fastq.gz"
    )
    GDC_UNMAPPED_FASTQ_SE_FILE = join(
        GDC_FILE_RESULTS_DIR, "{rg_id}_unmapped_se.fastq.gz"
    )
    GDC_UNMAPPED_FASTQ_LOG = join(GDC_FILE_LOG_DIR, "{rg_id}_unmapped_fastq.log")
    GDC_UNMAPPED_FASTQ_FILES = np.hstack(
        pd.concat(
            [
                GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] == 1].apply(
                    lambda x: (
                        [
                            join(
                                GDC_RESULTS_DIR,
                                "sg",
                                x["file_id"],
                                f"{x['file_id']}_unmapped_r1.fastq.gz",
                            ),
                            join(
                                GDC_RESULTS_DIR,
                                "sg",
                                x["file_id"],
                                f"{x['file_id']}_unmapped_r2.fastq.gz",
                            ),
                            join(
                                GDC_RESULTS_DIR,
                                "sg",
                                x["file_id"],
                                f"{x['file_id']}_unmapped_o1.fastq.gz",
                            ),
                            join(
                                GDC_RESULTS_DIR,
                                "sg",
                                x["file_id"],
                                f"{x['file_id']}_unmapped_o2.fastq.gz",
                            ),
                        ]
                        if x["is_paired_end"]
                        else [
                            join(
                                GDC_RESULTS_DIR,
                                "sg",
                                x["file_id"],
                                f"{x['file_id']}_unmapped_se.fastq.gz",
                            )
                        ]
                    ),
                    axis=1,
                ),
                GDC_READGRP_META_DF[
                    GDC_READGRP_META_DF["num_uniq_read_groups"] > 1
                ].apply(
                    lambda x: (
                        [
                            join(
                                GDC_RESULTS_DIR,
                                "rg",
                                x["file_id"],
                                f"{x['read_group_id']}_unmapped_r1.fastq.gz",
                            ),
                            join(
                                GDC_RESULTS_DIR,
                                "rg",
                                x["file_id"],
                                f"{x['read_group_id']}_unmapped_r2.fastq.gz",
                            ),
                            join(
                                GDC_RESULTS_DIR,
                                "rg",
                                x["file_id"],
                                f"{x['read_group_id']}_unmapped_o1.fastq.gz",
                            ),
                            join(
                                GDC_RESULTS_DIR,
                                "rg",
                                x["file_id"],
                                f"{x['read_group_id']}_unmapped_o2.fastq.gz",
                            ),
                        ]
                        if x["is_paired_end"]
                        else [
                            join(
                                GDC_RESULTS_DIR,
                                "rg",
                                x["file_id"],
                                f"{x['read_group_id']}_unmapped_se.fastq.gz",
                            )
                        ]
                    ),
                    axis=1,
                ),
            ],
            axis=0,
        )
    ).tolist()

BOWTIE2_RESULTS_DIR = join(RESULTS_DIR, "bowtie2")
BOWTIE2_LOG_DIR = join(LOG_DIR, "bowtie2")
BOWTIE2_HOST_INDEX_PREFIX = join(RESOURCES_DIR, "bowtie2", "genome")
BOWTIE2_HOST_INDEX_FILES = multiext(
    BOWTIE2_HOST_INDEX_PREFIX,
    ".1.bt2",
    ".2.bt2",
    ".3.bt2",
    ".4.bt2",
    ".rev.1.bt2",
    ".rev.2.bt2",
)
BOWTIE2_HOST_INDEX_LOG = join(BOWTIE2_LOG_DIR, "build", "index.log")
BOWTIE2_FILE_RESULTS_DIR = join(BOWTIE2_RESULTS_DIR, "{method}", "{bam_id}", "{etype}")
BOWTIE2_FILE_LOG_DIR = join(BOWTIE2_LOG_DIR, "align", "{method}", "{bam_id}", "{etype}")
BOWTIE2_SAM_FILE = join(BOWTIE2_FILE_RESULTS_DIR, "{rg_id}.sam")
BOWTIE2_FILTERED_SAM_FILE = join(BOWTIE2_FILE_RESULTS_DIR, "{rg_id}_filtered.sam")
BOWTIE2_ALIGN_LOG = join(BOWTIE2_FILE_LOG_DIR, "{rg_id}_align.log")
BOWTIE2_FILTERED_SAM_FILES = pd.concat(
    [
        GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] == 1].apply(
            lambda x: join(
                BOWTIE2_RESULTS_DIR,
                "sg",
                x["file_id"],
                "pe" if x["is_paired_end"] else "se",
                f"{x['file_id']}_filtered.sam",
            ),
            axis=1,
        ),
        GDC_READGRP_META_DF[GDC_READGRP_META_DF["num_uniq_read_groups"] > 1].apply(
            lambda x: join(
                BOWTIE2_RESULTS_DIR,
                "rg",
                x["file_id"],
                "pe" if x["is_paired_end"] else "se",
                f"{x['read_group_id']}_filtered.sam",
            ),
            axis=1,
        ),
    ],
    axis=0,
).tolist()
BOWTIE2_FILTERED_FASTQ_R1_FILE = join(
    BOWTIE2_FILE_RESULTS_DIR, "{rg_id}_filtered_r1.fastq.gz"
)
BOWTIE2_FILTERED_FASTQ_R2_FILE = join(
    BOWTIE2_FILE_RESULTS_DIR, "{rg_id}_filtered_r2.fastq.gz"
)
BOWTIE2_FILTERED_FASTQ_SE_FILE = join(
    BOWTIE2_FILE_RESULTS_DIR, "{rg_id}_filtered_se.fastq.gz"
)
BOWTIE2_FILTERED_FASTQ_LOG = join(BOWTIE2_FILE_LOG_DIR, "{rg_id}_filtered_fastq.log")
BOWTIE2_FILTERED_FASTQ_FILES = np.hstack(
    pd.concat(
        [
            GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] == 1].apply(
                lambda x: (
                    [
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "sg",
                            x["file_id"],
                            "pe",
                            f"{x['file_id']}_filtered_r1.fastq.gz",
                        ),
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "sg",
                            x["file_id"],
                            "pe",
                            f"{x['file_id']}_filtered_r2.fastq.gz",
                        ),
                    ]
                    if x["is_paired_end"]
                    else [
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "sg",
                            x["file_id"],
                            "se",
                            f"{x['file_id']}_filtered_se.fastq.gz",
                        )
                    ]
                ),
                axis=1,
            ),
            GDC_READGRP_META_DF[GDC_READGRP_META_DF["num_uniq_read_groups"] > 1].apply(
                lambda x: (
                    [
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "rg",
                            x["file_id"],
                            "pe",
                            f"{x['read_group_id']}_filtered_r1.fastq.gz",
                        ),
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "rg",
                            x["file_id"],
                            "pe",
                            f"{x['read_group_id']}_filtered_r2.fastq.gz",
                        ),
                    ]
                    if x["is_paired_end"]
                    else [
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "rg",
                            x["file_id"],
                            "se",
                            f"{x['read_group_id']}_filtered_se.fastq.gz",
                        )
                    ]
                ),
                axis=1,
            ),
        ],
        axis=0,
    )
).tolist()

KRAKENUNIQ_RESULTS_DIR = join(RESULTS_DIR, "krakenuniq")
KRAKENUNIQ_LOG_DIR = join(LOG_DIR, "krakenuniq")
KRAKENUNIQ_FILE_RESULTS_DIR = join(
    KRAKENUNIQ_RESULTS_DIR, "{method}", "{bam_id}", "{etype}"
)
KRAKENUNIQ_FILE_LOG_DIR = join(KRAKENUNIQ_LOG_DIR, "{method}", "{bam_id}", "{etype}")
KRAKENUNIQ_CLASSIF_FILE = join(KRAKENUNIQ_FILE_RESULTS_DIR, "{rg_id}_classif.tsv")
KRAKENUNIQ_REPORT_FILE = join(KRAKENUNIQ_FILE_RESULTS_DIR, "{rg_id}_report.tsv")
KRAKENUNIQ_CLASSIF_LOG = join(KRAKENUNIQ_FILE_LOG_DIR, "{rg_id}_classif.log")

READ_LENGTH_RESULTS_DIR = join(RESULTS_DIR, "read_length")
READ_LENGTH_LOG_DIR = join(LOG_DIR, "read_length")
READ_LENGTH_FILE_RESULTS_DIR = join(
    READ_LENGTH_RESULTS_DIR, "{method}", "{bam_id}", "{etype}"
)
READ_LENGTH_FILE_LOG_DIR = join(READ_LENGTH_LOG_DIR, "{method}", "{bam_id}", "{etype}")
READ_LENGTH_HISTOGRAM_FILE = join(READ_LENGTH_FILE_RESULTS_DIR, "{rg_id}_histogram.tsv")
READ_LENGTH_FILE = join(READ_LENGTH_FILE_RESULTS_DIR, "{rg_id}_length.txt")
READ_LENGTH_HISTOGRAM_LOG = join(READ_LENGTH_FILE_LOG_DIR, "{rg_id}_histogram.log")
READ_LENGTH_LOG = join(READ_LENGTH_FILE_LOG_DIR, "{rg_id}_length.log")

BRACKEN_RESULTS_DIR = join(RESULTS_DIR, "bracken")
BRACKEN_LOG_DIR = join(LOG_DIR, "bracken")

BRACKEN_COUNT_RESULTS_DIR = join(BRACKEN_RESULTS_DIR, "counts")
BRACKEN_COUNT_LOG_DIR = join(BRACKEN_LOG_DIR, "counts")
BRACKEN_COUNT_FILE_RESULTS_DIR = join(
    BRACKEN_COUNT_RESULTS_DIR, "{method}", "{bam_id}", "{etype}"
)
BRACKEN_COUNT_FILE_LOG_DIR = join(
    BRACKEN_COUNT_LOG_DIR, "{method}", "{bam_id}", "{etype}"
)
BRACKEN_COUNT_FILE = join(BRACKEN_COUNT_FILE_RESULTS_DIR, "{rg_id}_counts.tsv")
BRACKEN_REPORT_FILE = join(BRACKEN_COUNT_FILE_RESULTS_DIR, "{rg_id}_report.tsv")
BRACKEN_COUNT_LOG = join(BRACKEN_COUNT_FILE_LOG_DIR, "{rg_id}_counts.log")
BRACKEN_COUNT_FILES = pd.concat(
    [
        GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] == 1].apply(
            lambda x: join(
                BRACKEN_COUNT_RESULTS_DIR,
                "sg",
                x["file_id"],
                "pe" if x["is_paired_end"] else "se",
                f"{x['file_id']}_counts.tsv",
            ),
            axis=1,
        ),
        GDC_READGRP_META_DF[GDC_READGRP_META_DF["num_uniq_read_groups"] > 1].apply(
            lambda x: join(
                BRACKEN_COUNT_RESULTS_DIR,
                "rg",
                x["file_id"],
                "pe" if x["is_paired_end"] else "se",
                f"{x['read_group_id']}_counts.tsv",
            ),
            axis=1,
        ),
    ],
    axis=0,
).tolist()

BRACKEN_MERGED_RG_RESULTS_DIR = join(BRACKEN_RESULTS_DIR, "merge_rg")
BRACKEN_MERGED_RG_LOG_DIR = join(BRACKEN_LOG_DIR, "merge_rg")
BRACKEN_MERGED_RG_COUNT_FILE = join(
    BRACKEN_MERGED_RG_RESULTS_DIR, "{bam_id}_counts.tsv"
)
BRACKEN_MERGED_RG_COUNT_LOG = join(BRACKEN_MERGED_RG_LOG_DIR, "{bam_id}_counts.log")

BRACKEN_COUNT_MATRIX_RESULTS_DIR = join(BRACKEN_RESULTS_DIR, "matrix")
BRACKEN_COUNT_MATRIX_LOG_DIR = join(BRACKEN_LOG_DIR, "matrix")
BRACKEN_COUNT_MATRIX_FILE = join(
    BRACKEN_COUNT_MATRIX_RESULTS_DIR, f"{STUDY_NAME}_count_matrix.tsv"
)
BRACKEN_COUNT_MATRIX_LOG = join(
    BRACKEN_COUNT_MATRIX_LOG_DIR, f"{STUDY_NAME}_count_matrix.log"
)
BRACKEN_BAM_COUNT_FILES = pd.concat(
    [
        GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] == 1].apply(
            lambda x: join(
                BRACKEN_COUNT_RESULTS_DIR,
                "sg",
                x["file_id"],
                "pe" if x["is_paired_end"] else "se",
                f"{x['file_id']}_counts.tsv",
            ),
            axis=1,
        ),
        GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] > 1].apply(
            lambda x: join(
                BRACKEN_MERGED_RG_RESULTS_DIR,
                f"{x['file_id']}_counts.tsv",
            ),
            axis=1,
        ),
    ],
    axis=0,
).tolist()
BRACKEN_BAM_IDS = pd.concat(
    [
        GDC_BAM_META_DF.loc[GDC_BAM_META_DF["num_uniq_read_groups"] == 1, "file_id"],
        GDC_BAM_META_DF.loc[GDC_BAM_META_DF["num_uniq_read_groups"] > 1, "file_id"],
    ],
    axis=0,
).tolist()

BOWTIE2_BUILD_THREADS = (
    workflow.cores
    if config["bowtie2"]["build"]["threads"] == "all"
    else config["bowtie2"]["build"]["threads"]
)
BOWTIE2_ALIGN_THREADS = (
    workflow.cores
    if config["bowtie2"]["align"]["threads"] == "all"
    else config["bowtie2"]["align"]["threads"]
)
KRAKENUNIQ_THREADS = (
    workflow.cores
    if config["krakenuniq"]["threads"] == "all"
    else config["krakenuniq"]["threads"]
)

BIOBAMBAM2_BAMTOFASTQ_WRAPPER = join(
    LOCAL_WRAPPERS_BASE_URL, "bio/biobambam2/bamtofastq"
)
BOWTIE2_BUILD_WRAPPER = join(PUBLIC_WRAPPERS_VERSION, "bio/bowtie2/build")
BOWTIE2_ALIGN_WRAPPER = join(PUBLIC_WRAPPERS_VERSION, "bio/bowtie2/align")
KRAKENUNIQ_WRAPPER = join(LOCAL_WRAPPERS_BASE_URL, "bio/krakenuniq")
READ_LENGTH_WRAPPER = join(PERSONAL_WRAPPERS_BASE_URL, "bio/bbmap/readlength")
BRACKEN_WRAPPER = join(LOCAL_WRAPPERS_BASE_URL, "bio/bracken")


include: join(RULES_DIR, "gdc_file.smk")
include: join(RULES_DIR, "host_filter.smk")
include: join(RULES_DIR, "microbe.smk")
include: join(RULES_DIR, "read_length.smk")
include: join(RULES_DIR, "ref.smk")


wildcard_constraints:
    bam_id="[0-9a-f\\-]+",
    rg_id="[0-9a-f\\-]+",
    method="sg|rg",
    etype="pe|se",


rule all:
    input:
        # HOST_REF_FASTA_FILE,
        # GDC_UNMAPPED_BAM_FILES,
        # GDC_UNMAPPED_FASTQ_FILES,
        # BOWTIE2_HOST_INDEX_FILES,
        # BOWTIE2_FILTERED_SAM_FILES,
        # BOWTIE2_FILTERED_FASTQ_FILES,
        # BRACKEN_COUNT_FILES,
        BRACKEN_COUNT_MATRIX_FILE,


def clean(*dirs):
    for clean_dir in dirs:
        if exists(clean_dir):
            rmtree(clean_dir)
        for dirpath, dirnames, filenames in sorted(walk(getcwd())):
            for name in dirnames:
                if name == "__pycache__":
                    pycache_dir = join(dirpath, name)
                    if exists(pycache_dir):
                        rmtree(pycache_dir)


rule clean:
    run:
        clean(RESULTS_DIR, LOG_DIR)


rule clean_all:
    run:
        clean(RESOURCES_DIR, RESULTS_DIR, LOG_DIR)
