from os import getcwd, makedirs, walk
from os.path import basename, exists, join
from tempfile import gettempdir
from shutil import rmtree
from urllib.parse import urlparse

import numpy as np
import pandas as pd

CONFIG_DIR = "config"
DATA_DIR = "data"
LOG_DIR = "logs"
RESOURCES_DIR = "resources"
RESULTS_DIR = "results"
RULES_DIR = "rules"
REPORT_DIR = "report"


configfile: join(CONFIG_DIR, "config.yaml")


STUDY_NAME = config["study"]

TEMP_DIR = config["tmp_dir"] if config["tmp_dir"] else gettempdir()

EXPAND_PARAMS = {}

PUBLIC_WRAPPERS_VERSION = config["wrappers"]["public"]["version"]
PERSONAL_WRAPPERS_BASE_URL = config["wrappers"]["personal"]["base_url"]
LOCAL_WRAPPERS_BASE_URL = config["wrappers"]["local"]["base_url"]

HOST_REF_FASTA_URL = config["resources"]["ref"]["fasta_url"]
HOST_REF_FASTA_FILE = join(
    RESOURCES_DIR, "ref", basename(urlparse(HOST_REF_FASTA_URL).path)
)

GDC_METADATA_DIR = config["gdc"]["metadata"]["data_dir"]
GDC_METADATA_LOG_DIR = config["gdc"]["metadata"]["log_dir"]
GDC_BAM_META_FILENAME = config["gdc"]["metadata"]["file_meta_filename"]
GDC_BAM_META_FILE = join(GDC_METADATA_DIR, GDC_BAM_META_FILENAME)
GDC_READGRP_META_FILENAME = config["gdc"]["metadata"]["readgrp_meta_filename"]
GDC_READGRP_META_FILE = join(GDC_METADATA_DIR, GDC_READGRP_META_FILENAME)

KRAKEN2_DB_LIBS = config["resources"]["kraken"]["db"]["libs"]
KRAKEN2_DB_DIR = config["resources"]["kraken"]["db"]["dir"]
KRAKENUNIQ_DB_DIR = config["resources"]["kraken"]["db"]["dir"]
KRAKEN_MODE = config["read_classif"]["mode"]

EXPAND_PARAMS["klib"] = KRAKEN2_DB_LIBS

makedirs(GDC_METADATA_LOG_DIR, mode=0o755, exist_ok=True)
GDC_METADATA_LOG = join(GDC_METADATA_LOG_DIR, "gdc_metadata.log")
with open(GDC_METADATA_LOG, "wt") as log_fh:
    if not exists(GDC_BAM_META_FILE) or not exists(GDC_READGRP_META_FILE):
        log_fh.write("Getting GDC metadata...\n")
        makedirs(GDC_METADATA_DIR, mode=0o755, exist_ok=True)
    else:
        log_fh.write("Using existing GDC metadata\n")
if not exists(GDC_BAM_META_FILE) or not exists(GDC_READGRP_META_FILE):
    shell(f"Rscript ./workflow/scripts/gdc_metadata.R >> {GDC_METADATA_LOG} 2>&1")

GDC_BAM_META_DF = pd.read_csv(GDC_BAM_META_FILE, sep="\t").set_index(
    "file_id", drop=False, verify_integrity=True
)
GDC_READGRP_META_DF = pd.read_csv(
    GDC_READGRP_META_FILE, sep="\t", dtype={"is_paired_end": bool, "read_length": int}
).set_index("read_group_id", drop=False, verify_integrity=True)

GDC_TOKEN = os.getenv("GDC_TOKEN")
if GDC_TOKEN is None:
    GDC_TOKEN_FILE = os.getenv("GDC_TOKEN_FILE", "~/.gdc_token")
    if GDC_TOKEN_FILE is not None and exists(GDC_TOKEN_FILE):
        GDC_TOKEN_FILE = GDC_TOKEN_FILE.strip()
        with open(GDC_TOKEN_FILE, "rt") as token_fh:
            GDC_TOKEN = token_fh.readline().strip()
assert (
    GDC_TOKEN is not None
), "GDC auth token not found in GDC_TOKEN, GDC_TOKEN_FILE, or ~/.gdc_token"

GDC_RESULTS_DIR = join(RESULTS_DIR, "gdc")
GDC_LOG_DIR = join(LOG_DIR, "gdc")
GDC_FILE_RESULTS_DIR = join(GDC_RESULTS_DIR, "{method}", "{bam_id}")
GDC_FILE_LOG_DIR = join(GDC_LOG_DIR, "{method}", "{bam_id}")
GDC_UNMAPPED_BAM_FILE = join(GDC_FILE_RESULTS_DIR, "{bam_id}_unmapped.bam")
GDC_UNMAPPED_BAM_LOG = join(GDC_FILE_LOG_DIR, "{bam_id}_unmapped_bam.log")
GDC_UNMAPPED_BAM_FILES = GDC_BAM_META_DF.apply(
    lambda x: (
        join(
            GDC_RESULTS_DIR,
            "sg",
            x["file_id"],
            f"{x['file_id']}_unmapped.bam",
        )
        if x["num_uniq_read_groups"] == 1
        else join(
            GDC_RESULTS_DIR,
            "rg",
            x["file_id"],
            f"{x['file_id']}_unmapped.bam",
        )
    ),
    axis=1,
).tolist()
GDC_UNMAPPED_FASTQ_R1_FILE = join(GDC_FILE_RESULTS_DIR, "{rg_id}_unmapped_r1.fastq.gz")
GDC_UNMAPPED_FASTQ_R2_FILE = join(GDC_FILE_RESULTS_DIR, "{rg_id}_unmapped_r2.fastq.gz")
GDC_UNMAPPED_FASTQ_O1_FILE = join(GDC_FILE_RESULTS_DIR, "{rg_id}_unmapped_o1.fastq.gz")
GDC_UNMAPPED_FASTQ_O2_FILE = join(GDC_FILE_RESULTS_DIR, "{rg_id}_unmapped_o2.fastq.gz")
GDC_UNMAPPED_FASTQ_SE_FILE = join(GDC_FILE_RESULTS_DIR, "{rg_id}_unmapped_se.fastq.gz")
GDC_UNMAPPED_FASTQ_LOG = join(GDC_FILE_LOG_DIR, "{rg_id}_unmapped_fastq.log")
GDC_UNMAPPED_FASTQ_FILES = np.hstack(
    pd.concat(
        [
            GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] == 1].apply(
                lambda x: (
                    [
                        join(
                            GDC_RESULTS_DIR,
                            "sg",
                            x["file_id"],
                            f"{x['file_id']}_unmapped_r1.fastq.gz",
                        ),
                        join(
                            GDC_RESULTS_DIR,
                            "sg",
                            x["file_id"],
                            f"{x['file_id']}_unmapped_r2.fastq.gz",
                        ),
                        join(
                            GDC_RESULTS_DIR,
                            "sg",
                            x["file_id"],
                            f"{x['file_id']}_unmapped_o1.fastq.gz",
                        ),
                        join(
                            GDC_RESULTS_DIR,
                            "sg",
                            x["file_id"],
                            f"{x['file_id']}_unmapped_o2.fastq.gz",
                        ),
                    ]
                    if x["is_paired_end"]
                    else [
                        join(
                            GDC_RESULTS_DIR,
                            "sg",
                            x["file_id"],
                            f"{x['file_id']}_unmapped_se.fastq.gz",
                        )
                    ]
                ),
                axis=1,
            ),
            GDC_READGRP_META_DF[GDC_READGRP_META_DF["num_uniq_read_groups"] > 1].apply(
                lambda x: (
                    [
                        join(
                            GDC_RESULTS_DIR,
                            "rg",
                            x["file_id"],
                            f"{x['read_group_id']}_unmapped_r1.fastq.gz",
                        ),
                        join(
                            GDC_RESULTS_DIR,
                            "rg",
                            x["file_id"],
                            f"{x['read_group_id']}_unmapped_r2.fastq.gz",
                        ),
                        join(
                            GDC_RESULTS_DIR,
                            "rg",
                            x["file_id"],
                            f"{x['read_group_id']}_unmapped_o1.fastq.gz",
                        ),
                        join(
                            GDC_RESULTS_DIR,
                            "rg",
                            x["file_id"],
                            f"{x['read_group_id']}_unmapped_o2.fastq.gz",
                        ),
                    ]
                    if x["is_paired_end"]
                    else [
                        join(
                            GDC_RESULTS_DIR,
                            "rg",
                            x["file_id"],
                            f"{x['read_group_id']}_unmapped_se.fastq.gz",
                        )
                    ]
                ),
                axis=1,
            ),
        ],
        axis=0,
    )
).tolist()

BOWTIE2_RESULTS_DIR = join(RESULTS_DIR, "bowtie2")
BOWTIE2_LOG_DIR = join(LOG_DIR, "bowtie2")
BOWTIE2_HOST_INDEX_PREFIX = join(RESOURCES_DIR, "bowtie2", "genome")
BOWTIE2_HOST_INDEX_FILES = multiext(
    BOWTIE2_HOST_INDEX_PREFIX,
    ".1.bt2",
    ".2.bt2",
    ".3.bt2",
    ".4.bt2",
    ".rev.1.bt2",
    ".rev.2.bt2",
)
BOWTIE2_HOST_INDEX_LOG = join(BOWTIE2_LOG_DIR, "build", "index.log")
BOWTIE2_FILE_RESULTS_DIR = join(BOWTIE2_RESULTS_DIR, "{method}", "{bam_id}", "{etype}")
BOWTIE2_FILE_LOG_DIR = join(BOWTIE2_LOG_DIR, "align", "{method}", "{bam_id}", "{etype}")
BOWTIE2_SAM_FILE = join(BOWTIE2_FILE_RESULTS_DIR, "{rg_id}.sam")
BOWTIE2_FILTERED_SAM_FILE = join(BOWTIE2_FILE_RESULTS_DIR, "{rg_id}_filtered.sam")
BOWTIE2_ALIGN_LOG = join(BOWTIE2_FILE_LOG_DIR, "{rg_id}_align.log")
BOWTIE2_FILTERED_SAM_FILES = pd.concat(
    [
        GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] == 1].apply(
            lambda x: join(
                BOWTIE2_RESULTS_DIR,
                "sg",
                x["file_id"],
                "pe" if x["is_paired_end"] else "se",
                f"{x['file_id']}_filtered.sam",
            ),
            axis=1,
        ),
        GDC_READGRP_META_DF[GDC_READGRP_META_DF["num_uniq_read_groups"] > 1].apply(
            lambda x: join(
                BOWTIE2_RESULTS_DIR,
                "rg",
                x["file_id"],
                "pe" if x["is_paired_end"] else "se",
                f"{x['read_group_id']}_filtered.sam",
            ),
            axis=1,
        ),
    ],
    axis=0,
).tolist()
BOWTIE2_FILTERED_FASTQ_R1_FILE = join(
    BOWTIE2_FILE_RESULTS_DIR, "{rg_id}_filtered_r1.fastq.gz"
)
BOWTIE2_FILTERED_FASTQ_R2_FILE = join(
    BOWTIE2_FILE_RESULTS_DIR, "{rg_id}_filtered_r2.fastq.gz"
)
BOWTIE2_FILTERED_FASTQ_SE_FILE = join(
    BOWTIE2_FILE_RESULTS_DIR, "{rg_id}_filtered_se.fastq.gz"
)
BOWTIE2_FILTERED_FASTQ_LOG = join(BOWTIE2_FILE_LOG_DIR, "{rg_id}_filtered_fastq.log")
BOWTIE2_FILTERED_FASTQ_FILES = np.hstack(
    pd.concat(
        [
            GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] == 1].apply(
                lambda x: (
                    [
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "sg",
                            x["file_id"],
                            "pe",
                            f"{x['file_id']}_filtered_r1.fastq.gz",
                        ),
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "sg",
                            x["file_id"],
                            "pe",
                            f"{x['file_id']}_filtered_r2.fastq.gz",
                        ),
                    ]
                    if x["is_paired_end"]
                    else [
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "sg",
                            x["file_id"],
                            "se",
                            f"{x['file_id']}_filtered_se.fastq.gz",
                        )
                    ]
                ),
                axis=1,
            ),
            GDC_READGRP_META_DF[GDC_READGRP_META_DF["num_uniq_read_groups"] > 1].apply(
                lambda x: (
                    [
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "rg",
                            x["file_id"],
                            "pe",
                            f"{x['read_group_id']}_filtered_r1.fastq.gz",
                        ),
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "rg",
                            x["file_id"],
                            "pe",
                            f"{x['read_group_id']}_filtered_r2.fastq.gz",
                        ),
                    ]
                    if x["is_paired_end"]
                    else [
                        join(
                            BOWTIE2_RESULTS_DIR,
                            "rg",
                            x["file_id"],
                            "se",
                            f"{x['read_group_id']}_filtered_se.fastq.gz",
                        )
                    ]
                ),
                axis=1,
            ),
        ],
        axis=0,
    )
).tolist()

KRAKEN2_RESOURCES_DIR = join(RESOURCES_DIR, "kraken2")
KRAKEN2_LOG_DIR = join(LOG_DIR, "kraken2")
KRAKEN2_DB_TAX_DIR = join(KRAKEN2_DB_DIR, "taxonomy")
KRAKEN2_DB_TAX_LOG = join(KRAKEN2_LOG_DIR, "taxonomy.log")
KRAKEN2_DB_LIB_DIR = join(KRAKEN2_DB_DIR, "library", "{klib}")
KRAKEN2_DB_LIB_LOG = join(KRAKEN2_LOG_DIR, "{klib}.log")
KRAKEN2_DB_BUILD_LOG = join(KRAKEN2_LOG_DIR, "kraken2_build.log")
KRAKEN2_DB_TAX_TOUCH_FILE = join(KRAKEN2_DB_TAX_DIR, "kraken2_db_taxonomy.done")
KRAKEN2_DB_LIB_TOUCH_FILE = join(KRAKEN2_DB_LIB_DIR, "kraken2_db_{klib}.done")
KRAKEN2_DB_BUILD_TOUCH_FILE = join(KRAKEN2_DB_DIR, "kraken2_db_build.done")
BRACKEN_DB_BUILD_TOUCH_FILE = join(KRAKEN2_DB_DIR, "bracken_db_build.done")
BRACKEN_DB_BUILD_LOG = join(KRAKEN2_LOG_DIR, "bracken_build.log")

KRAKENUNIQ_RESULTS_DIR = join(RESULTS_DIR, "krakenuniq")
KRAKENUNIQ_LOG_DIR = join(LOG_DIR, "krakenuniq")
KRAKENUNIQ_FILE_RESULTS_DIR = join(
    KRAKENUNIQ_RESULTS_DIR, "{method}", "{bam_id}", "{etype}"
)
KRAKENUNIQ_FILE_LOG_DIR = join(KRAKENUNIQ_LOG_DIR, "{method}", "{bam_id}", "{etype}")
KRAKENUNIQ_CLASSIF_FILE = join(KRAKENUNIQ_FILE_RESULTS_DIR, "{rg_id}_classif.tsv")
KRAKENUNIQ_REPORT_FILE = join(KRAKENUNIQ_FILE_RESULTS_DIR, "{rg_id}_report.tsv")
KRAKENUNIQ_CLASSIF_LOG = join(KRAKENUNIQ_FILE_LOG_DIR, "{rg_id}_classif.log")

BRACKEN_RESULTS_DIR = join(RESULTS_DIR, "bracken")
BRACKEN_LOG_DIR = join(LOG_DIR, "bracken")
BRACKEN_COUNT_RESULTS_DIR = join(BRACKEN_RESULTS_DIR, "counts")
BRACKEN_COUNT_LOG_DIR = join(BRACKEN_LOG_DIR, "counts")
BRACKEN_COUNT_FILE_RESULTS_DIR = join(
    BRACKEN_COUNT_RESULTS_DIR, "{method}", "{bam_id}", "{etype}"
)
BRACKEN_COUNT_FILE_LOG_DIR = join(
    BRACKEN_COUNT_LOG_DIR, "{method}", "{bam_id}", "{etype}"
)
BRACKEN_COUNT_FILE = join(BRACKEN_COUNT_FILE_RESULTS_DIR, "{rg_id}_counts.tsv")
BRACKEN_REPORT_FILE = join(BRACKEN_COUNT_FILE_RESULTS_DIR, "{rg_id}_report.tsv")
BRACKEN_COUNT_LOG = join(BRACKEN_COUNT_FILE_LOG_DIR, "{rg_id}_counts.log")
BRACKEN_COUNT_FILES = pd.concat(
    [
        GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] == 1].apply(
            lambda x: join(
                BRACKEN_COUNT_RESULTS_DIR,
                "sg",
                x["file_id"],
                "pe" if x["is_paired_end"] else "se",
                f"{x['file_id']}_counts.tsv",
            ),
            axis=1,
        ),
        GDC_READGRP_META_DF[GDC_READGRP_META_DF["num_uniq_read_groups"] > 1].apply(
            lambda x: join(
                BRACKEN_COUNT_RESULTS_DIR,
                "rg",
                x["file_id"],
                "pe" if x["is_paired_end"] else "se",
                f"{x['read_group_id']}_counts.tsv",
            ),
            axis=1,
        ),
    ],
    axis=0,
).tolist()

BRACKEN_MERGED_RG_RESULTS_DIR = join(BRACKEN_RESULTS_DIR, "merge_rg")
BRACKEN_MERGED_RG_LOG_DIR = join(BRACKEN_LOG_DIR, "merge_rg")
BRACKEN_MERGED_RG_COUNT_FILE = join(
    BRACKEN_MERGED_RG_RESULTS_DIR, "{bam_id}_counts.tsv"
)
BRACKEN_MERGED_RG_COUNT_LOG = join(BRACKEN_MERGED_RG_LOG_DIR, "{bam_id}_counts.log")

BRACKEN_COUNT_MATRIX_RESULTS_DIR = join(BRACKEN_RESULTS_DIR, "matrix")
BRACKEN_COUNT_MATRIX_LOG_DIR = join(BRACKEN_LOG_DIR, "matrix")
BRACKEN_COUNT_MATRIX_FILE = join(
    BRACKEN_COUNT_MATRIX_RESULTS_DIR, f"{STUDY_NAME}_count_matrix.tsv"
)
BRACKEN_COUNT_MATRIX_LOG = join(
    BRACKEN_COUNT_MATRIX_LOG_DIR, f"{STUDY_NAME}_count_matrix.log"
)
BRACKEN_BAM_COUNT_FILES = pd.concat(
    [
        GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] == 1].apply(
            lambda x: join(
                BRACKEN_COUNT_RESULTS_DIR,
                "sg",
                x["file_id"],
                "pe" if x["is_paired_end"] else "se",
                f"{x['file_id']}_counts.tsv",
            ),
            axis=1,
        ),
        GDC_BAM_META_DF[GDC_BAM_META_DF["num_uniq_read_groups"] > 1].apply(
            lambda x: join(
                BRACKEN_MERGED_RG_RESULTS_DIR,
                f"{x['file_id']}_counts.tsv",
            ),
            axis=1,
        ),
    ],
    axis=0,
).tolist()
BRACKEN_BAM_IDS = pd.concat(
    [
        GDC_BAM_META_DF.loc[GDC_BAM_META_DF["num_uniq_read_groups"] == 1, "file_id"],
        GDC_BAM_META_DF.loc[GDC_BAM_META_DF["num_uniq_read_groups"] > 1, "file_id"],
    ],
    axis=0,
).tolist()

BOWTIE2_BUILD_THREADS = (
    workflow.cores
    if config["bowtie2"]["build"]["threads"] == "all"
    else config["bowtie2"]["build"]["threads"]
)
BOWTIE2_ALIGN_THREADS = (
    workflow.cores
    if config["bowtie2"]["align"]["threads"] == "all"
    else config["bowtie2"]["align"]["threads"]
)
KRAKEN2_BUILD_THREADS = (
    workflow.cores
    if config["kraken2"]["build"]["threads"] == "all"
    else config["kraken2"]["build"]["threads"]
)
BRACKEN_BUILD_THREADS = (
    workflow.cores
    if config["bracken"]["build"]["threads"] == "all"
    else config["bracken"]["build"]["threads"]
)
KRAKENUNIQ_THREADS = (
    workflow.cores
    if config["krakenuniq"]["threads"] == "all"
    else config["krakenuniq"]["threads"]
)

BIOBAMBAM2_BAMTOFASTQ_WRAPPER = join(
    LOCAL_WRAPPERS_BASE_URL, "bio/biobambam2/bamtofastq"
)
BOWTIE2_BUILD_WRAPPER = join(PUBLIC_WRAPPERS_VERSION, "bio/bowtie2/build")
BOWTIE2_ALIGN_WRAPPER = join(PUBLIC_WRAPPERS_VERSION, "bio/bowtie2/align")
KRAKEN2_BUILD_WRAPPER = join(LOCAL_WRAPPERS_BASE_URL, "bio/kraken2/build")
KRAKENUNIQ_WRAPPER = join(LOCAL_WRAPPERS_BASE_URL, "bio/krakenuniq")
BRACKEN_BUILD_WRAPPER = join(LOCAL_WRAPPERS_BASE_URL, "bio/bracken/build")
BRACKEN_QUANT_WRAPPER = join(LOCAL_WRAPPERS_BASE_URL, "bio/bracken/quant")


include: join(RULES_DIR, "gdc_file.smk")
include: join(RULES_DIR, "host_filter.smk")
include: join(RULES_DIR, "read_classif.smk")
include: join(RULES_DIR, "read_quant.smk")


wildcard_constraints:
    **{w: "|".join(set([re.escape(v) for v in l])) for w, l in EXPAND_PARAMS.items()},
    bam_id="[0-9a-f\\-]+",
    rg_id="[0-9a-f\\-]+",
    method="sg|rg",
    etype="pe|se",


rule all:
    input:
        # HOST_REF_FASTA_FILE,
        KRAKEN2_DB_TAX_TOUCH_FILE,
        expand(KRAKEN2_DB_LIB_TOUCH_FILE, **EXPAND_PARAMS),
        KRAKEN2_DB_BUILD_TOUCH_FILE,
        # GDC_UNMAPPED_BAM_FILES,
        # GDC_UNMAPPED_FASTQ_FILES,
        # BOWTIE2_HOST_INDEX_FILES,
        # BOWTIE2_FILTERED_SAM_FILES,
        # BOWTIE2_FILTERED_FASTQ_FILES,
        # BRACKEN_COUNT_FILES,
        BRACKEN_COUNT_MATRIX_FILE,


def clean(*dirs):
    for clean_dir in dirs:
        if exists(clean_dir):
            rmtree(clean_dir)
        for dirpath, dirnames, filenames in sorted(walk(getcwd())):
            for name in dirnames:
                if name == "__pycache__":
                    pycache_dir = join(dirpath, name)
                    if exists(pycache_dir):
                        rmtree(pycache_dir)


rule clean:
    run:
        clean(RESULTS_DIR, LOG_DIR)


rule clean_all:
    run:
        clean(RESOURCES_DIR, RESULTS_DIR, LOG_DIR)
