from os import getcwd, makedirs, walk
from os.path import basename, exists, join
from tempfile import gettempdir
from shutil import rmtree
from urllib.parse import urlparse

import numpy as np
import pandas as pd

CONFIG_DIR = "config"
DATA_DIR = "data"
LOG_DIR = "logs"
RESOURCES_DIR = "resources"
RESULTS_DIR = "results"
RULES_DIR = "rules"
REPORT_DIR = "report"


configfile: join(CONFIG_DIR, "config.yaml")


STUDY_NAME = config["study"]

TEMP_DIR = config["tmp_dir"] if config["tmp_dir"] else gettempdir()

EXPAND_PARAMS = {}

PUBLIC_WRAPPERS_VERSION = config["wrappers"]["public"]["version"]
PERSONAL_WRAPPERS_BASE_URL = config["wrappers"]["personal"]["base_url"]

INPUT_MODE = config["input"]["mode"]

REF_FASTA_URL = config["resources"]["ref"]["fasta_url"]
REF_FASTA_FILE = join(RESOURCES_DIR, "ref", basename(urlparse(REF_FASTA_URL).path))

GDC_METADATA_DIR = config["input"]["gdc"]["metadata"]["dir"]
if INPUT_MODE == "gdc":
    GDC_BAM_META_FILENAME = config["input"]["gdc"]["metadata"]["file_meta_filename"]
    GDC_BAM_META_FILE = join(GDC_METADATA_DIR, GDC_BAM_META_FILENAME)
    GDC_READGRP_META_FILENAME = config["input"]["gdc"]["metadata"][
        "readgrp_meta_filename"
    ]
    GDC_READGRP_META_FILE = join(GDC_METADATA_DIR, GDC_READGRP_META_FILENAME)

    if not exists(GDC_BAM_META_FILE) or not exists(GDC_READGRP_META_FILE):
        print("Getting GDC metadata...")
        makedirs(GDC_METADATA_DIR, mode=0o755, exist_ok=True)
        shell("Rscript ./workflow/scripts/gdc_metadata.R")
    else:
        print("Using existing GDC metadata")

    GDC_BAM_META_DF = pd.read_csv(GDC_BAM_META_FILE, sep="\t").set_index(
        "file_id", drop=False, verify_integrity=True
    )
    GDC_READGRP_META_DF = pd.read_csv(
        GDC_READGRP_META_FILE, sep="\t", dtype={"is_paired_end": bool}
    ).set_index("read_group_id", drop=False, verify_integrity=True)
    GDC_BAM_ID_WILDCARD_STR = "{bam_id}"
    GDC_BAM_IDS = GDC_BAM_META_DF["file_id"].tolist()
    EXPAND_PARAMS["bam_id"] = GDC_BAM_IDS
    GDC_READGRP_ID_WILDCARD_STR = "{rg_id}"
    GDC_READGRP_IDS = GDC_READGRP_META_DF["read_group_id"].tolist()
    EXPAND_PARAMS["rg_id"] = GDC_READGRP_IDS

    GDC_RESULTS_DIR = join(RESULTS_DIR, "gdc")
    GDC_LOG_DIR = join(LOG_DIR, "gdc")
    GDC_BAM_META_LOG = join(GDC_LOG_DIR, "gdc_bam_meta.log")
    GDC_UNMAPPED_BAM_FILE = join(GDC_RESULTS_DIR, "{bam_id}_unmapped.bam")
    GDC_UNMAPPED_BAM_LOG = join(GDC_LOG_DIR, "{bam_id}_unmapped_bam.log")
    GDC_UNMAPPED_FASTQ_1_FILE = join(GDC_RESULTS_DIR, "{rg_id}_unmapped_1.fastq.gz")
    GDC_UNMAPPED_FASTQ_2_FILE = join(GDC_RESULTS_DIR, "{rg_id}_unmapped_2.fastq.gz")
    GDC_UNMAPPED_FASTQ_S_FILE = join(GDC_RESULTS_DIR, "{rg_id}_unmapped_s.fastq.gz")
    GDC_UNMAPPED_FASTQ_LOG = join(GDC_LOG_DIR, "{rg_id}_unmapped_fastq.log")
    GDC_UNMAPPED_FASTQ_FILES = np.hstack(
        GDC_READGRP_META_DF.apply(
            lambda rg: (
                [
                    f"{rg.read_group_id}_unmapped_1.fastq.gz",
                    f"{rg.read_group_id}_unmapped_2.fastq.gz",
                ]
                if rg.is_paired_end
                else [f"{rg.read_group_id}_unmapped_s.fastq.gz"]
            ),
            axis=1,
        )
    ).tolist()

BOWTIE2_RESULTS_DIR = join(RESULTS_DIR, "bowtie2")
BOWTIE2_LOG_DIR = join(LOG_DIR, "bowtie2")
BOWTIE2_INDEX_DIR = join(RESOURCES_DIR, "bowtie2", "index")
BOWTIE2_INDEX_LOG = join(BOWTIE2_LOG_DIR, "genome_index.log")
BOWTIE2_SAM_FILE = join(BOWTIE2_RESULTS_DIR, "{rg_id}.sam")
BOWTIE2_FILTERED_SAM_FILE = join(BOWTIE2_RESULTS_DIR, "{rg_id}_filtered.sam")
BOWTIE2_ALIGN_LOG = join(BOWTIE2_LOG_DIR, "{rg_id}.log")
BOWTIE2_SORTED_FILTERED_SAM_FILE = join(
    BOWTIE2_RESULTS_DIR, "{rg_id}_filtered_sorted.sam"
)
BOWTIE2_SORTED_FILTERED_SAM_LOG = join(BOWTIE2_LOG_DIR, "{rg_id}_filtered_sorted.log")
BOWTIE2_FILTERED_FASTQ_1_FILE = join(BOWTIE2_RESULTS_DIR, "{rg_id}_filtered_1.fastq.gz")
BOWTIE2_FILTERED_FASTQ_2_FILE = join(BOWTIE2_RESULTS_DIR, "{rg_id}_filtered_2.fastq.gz")
BOWTIE2_FILTERED_FASTQ_S_FILE = join(BOWTIE2_RESULTS_DIR, "{rg_id}_filtered_s.fastq.gz")
BOWTIE2_FILTERED_FASTQ_LOG = join(BOWTIE2_LOG_DIR, "{rg_id}_filtered_fastq.log")
BOWTIE2_FILTERED_FASTQ_FILES = np.hstack(
    GDC_READGRP_META_DF.apply(
        lambda rg: (
            [
                f"{rg.read_group_id}_filtered_1.fastq.gz",
                f"{rg.read_group_id}_filtered_2.fastq.gz",
            ]
            if rg.is_paired_end
            else [f"{rg.read_group_id}_filtered_s.fastq.gz"]
        ),
        axis=1,
    )
).tolist()

KRAKENUNIQ_RESULTS_DIR = join(RESULTS_DIR, "krakenuniq")
KRAKENUNIQ_LOG_DIR = join(LOG_DIR, "krakenuniq")
KRAKENUNIQ_MICROBIALDB_CLASSIF_FILE = join(
    KRAKENUNIQ_RESULTS_DIR, "{rg_id}_classif.tsv"
)
KRAKENUNIQ_MICROBIALDB_REPORT_FILE = join(KRAKENUNIQ_RESULTS_DIR, "{rg_id}_report.txt")
KRAKENUNIQ_MICROBIALDB_CLASSIF_LOG = join(KRAKENUNIQ_LOG_DIR, "{rg_id}_classif.log")

BRACKEN_RESULTS_DIR = join(RESULTS_DIR, "bracken")
BRACKEN_LOG_DIR = join(LOG_DIR, "bracken")
BRACKEN_MICROBIALDB_QUANT_FILE = join(BRACKEN_RESULTS_DIR, "{rg_id}_quant.tsv")
BRACKEN_MICROBIALDB_QUANT_LOG = join(BRACKEN_LOG_DIR, "{rg_id}_classif.log")

SAMTOOLS_SORT_THREADS = (
    workflow.cores
    if config["samtools"]["sort"]["threads"] == "all"
    else config["samtools"]["sort"]["threads"]
)
SAMTOOLS_FASTQ_THREADS = (
    workflow.cores
    if config["samtools"]["fastq"]["threads"] == "all"
    else config["samtools"]["fastq"]["threads"]
)
BOWTIE2_BUILD_THREADS = (
    workflow.cores
    if config["bowtie2"]["build"]["threads"] == "all"
    else config["bowtie2"]["build"]["threads"]
)
BOWTIE2_ALIGN_THREADS = (
    workflow.cores
    if config["bowtie2"]["align"]["threads"] == "all"
    else config["bowtie2"]["align"]["threads"]
)
KRAKENUNIQ_THREADS = (
    workflow.cores
    if config["krakenuniq"]["threads"] == "all"
    else config["krakenuniq"]["threads"]
)

BIOBAMBAM_BAMTOFASTQ_WRAPPER = join(PERSONAL_WRAPPERS_BASE_URL, "bio/biobambam2")
SAMTOOLS_SORT_WRAPPER = join(PUBLIC_WRAPPERS_VERSION, "bio/samtools/sort")
SAMTOOLS_FASTQ_SEPARATE_WRAPPER = join(
    PUBLIC_WRAPPERS_VERSION, "bio/samtools/fastq/separate"
)
BOWTIE2_BUILD_WRAPPER = join(PUBLIC_WRAPPERS_VERSION, "bio/bowtie2/build")
BOWTIE2_ALIGN_WRAPPER = join(PUBLIC_WRAPPERS_VERSION, "bio/bowtie2/align")
KRAKENUNIQ_WRAPPER = join(PERSONAL_WRAPPERS_BASE_URL, "bio/krakenuniq")
BRACKEN_WRAPPER = join(PERSONAL_WRAPPERS_BASE_URL, "bio/bracken")


include: join(RULES_DIR, "gdc_file.smk")
include: join(RULES_DIR, "host_filter.smk")
include: join(RULES_DIR, "microbe.smk")
include: join(RULES_DIR, "ref.smk")


wildcard_constraints:
    **{w: "|".join(set([re.escape(v) for v in l])) for w, l in EXPAND_PARAMS.items()},


rule all:
    input:
        REF_FASTA_FILE,
        expand(GDC_UNMAPPED_BAM_FILE, **EXPAND_PARAMS),
        GDC_UNMAPPED_FASTQ_FILES,
        BOWTIE2_INDEX_DIR,
        expand(BOWTIE2_FILTERED_SAM_FILE, **EXPAND_PARAMS),
        expand(BOWTIE2_SORTED_FILTERED_SAM_FILE, **EXPAND_PARAMS),
        BOWTIE2_FILTERED_FASTQ_FILES,


def clean(*dirs):
    for clean_dir in dirs:
        if exists(clean_dir):
            rmtree(clean_dir)
        for dirpath, dirnames, filenames in sorted(walk(getcwd())):
            for name in dirnames:
                if name == "__pycache__":
                    pycache_dir = join(dirpath, name)
                    if exists(pycache_dir):
                        rmtree(pycache_dir)


rule clean:
    run:
        clean(RESULTS_DIR, LOG_DIR)


rule clean_all:
    run:
        clean(RESOURCES_DIR, RESULTS_DIR, LOG_DIR)
